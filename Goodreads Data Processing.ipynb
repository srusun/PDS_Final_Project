{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yfmkhh2aWg8dnQyvnoRXg\n"
     ]
    }
   ],
   "source": [
    "def read_api_key(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.read().replace('\\n','')\n",
    "    \n",
    "api_key = read_api_key('api_key.txt')\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for querying goodreads API\n",
    "def goodreads_query(api_key):\n",
    "    \"\"\"\n",
    "    api_key (string): API key\n",
    "    url (string): Base query URL\n",
    "    query (dictionary): Query parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    request_params = {'id':68428, 'key':api_key, 'format':'xml', 'text_only':'true'}\n",
    "    response = requests.get('https://www.goodreads.com/book/show', params=request_params)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    shelves = soup.find('popular_shelves')\n",
    "    return response\n",
    "    \n",
    "temp_response = goodreads_query(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata about a book given its book ID\n",
    "def get_book_metadata(api_key, book_id):\n",
    "    request_params = {'id':book_id, 'key':api_key, 'format':'xml', 'text_only':'true'}\n",
    "    response = requests.get('https://www.goodreads.com/book/show', params=request_params)\n",
    "    \n",
    "    book_info = {}\n",
    "    \n",
    "    book_info['book_id'] = book_id\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Author\n",
    "    book_info['author'] = soup.find('author').find('name').text\n",
    "    \n",
    "    # Publication date\n",
    "    year = soup.find('publication_year').text\n",
    "    month = soup.find('publication_month').text\n",
    "    day = soup.find('publication_day').text\n",
    "    book_info['publication_date'] = str(month) + '/' + str(day) + '/' + str(year)\n",
    "    \n",
    "    # Publisher\n",
    "    book_info['publisher'] = soup.find('publisher').text\n",
    "    \n",
    "    # Review information\n",
    "    book_info['review_count'] = soup.find('text_reviews_count').text\n",
    "    \n",
    "    # Rating information\n",
    "    book_info['rating_count'] = soup.find('ratings_count').text\n",
    "    book_info['average_rating'] = soup.find('average_rating').text\n",
    "    \n",
    "    # Treat the genre as the most popular shelf this book has been placed on\n",
    "    book_info['genre'] = soup.find('popular_shelves').find('shelf')['name']\n",
    "    \n",
    "    return book_info\n",
    "\n",
    "# Save all book metadata as a CSV\n",
    "def download_book_metadata(api_key):\n",
    "    data_columns = ['book_id', 'author', 'publication_date', 'publisher', 'review_count', 'rating_count', 'average_rating', 'genre']\n",
    "    \n",
    "    with open('metadata.csv', 'w', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, data_columns, lineterminator='\\n')\n",
    "        dict_writer.writeheader()\n",
    "        \n",
    "        # TODO: iterate through book IDs - add a time.sleep(1) between each iteration\n",
    "        current_metadata = get_book_metadata(api_key, 68428)\n",
    "        dict_writer.writerow(current_metadata)\n",
    "\n",
    "download_book_metadata(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the reviews for a given book ID as a list of dictionaries\n",
    "# Review data we are collecting - rating, date, and review text\n",
    "def get_reviews(api_key, book_id):\n",
    "    request_params = {'id':book_id, 'key':api_key, 'format':'xml', 'text_only':'true'}\n",
    "    response = requests.get('https://www.goodreads.com/book/show', params=request_params)\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    iframe = soup.find('reviews_widget').find('iframe')\n",
    "    \n",
    "    all_reviews = []\n",
    "    \n",
    "    reviews_url = iframe['src']\n",
    "    response_reviews = requests.get(reviews_url)\n",
    "    \n",
    "    while(1):\n",
    "        review_soup = BeautifulSoup(response_reviews.text, 'lxml')\n",
    "        reviews = review_soup.find_all('div', {'class':'gr_review_container'})\n",
    "\n",
    "        for review in reviews:\n",
    "            current_review = {}\n",
    "\n",
    "            # Extract the rating\n",
    "            rating = review.find('span', {'class':'gr_rating'})\n",
    "            if (rating):\n",
    "                rating = rating.find_all(text=True)\n",
    "                rating_num = 0\n",
    "                for i in range(len(rating[0])):\n",
    "                    if (ord(rating[0][i]) == 9733):\n",
    "                        rating_num += 1\n",
    "            else:\n",
    "                rating_num = -1\n",
    "\n",
    "            # Extract the date\n",
    "            date = review.find('span', {'class':'gr_review_date'}).find_all(text=True)\n",
    "            date = [x.strip() for x in date][0]\n",
    "\n",
    "            # Extract the review text\n",
    "            s = [x.strip() for x in review.find('div', {'class':'gr_review_text'}).find_all(text=True)]\n",
    "            s = [x for x in s if x]\n",
    "\n",
    "            # Ignore the last element, the '...more'\n",
    "            review_text = ' '.join(s[:-1])\n",
    "\n",
    "            # Remove the last word, since it will be partial\n",
    "            review_text = ' '.join(review_text.split(' ')[:-1])\n",
    "\n",
    "            current_review['book_id'] = book_id\n",
    "            current_review['rating'] = rating_num\n",
    "            current_review['date'] = date\n",
    "            current_review['text'] = review_text\n",
    "\n",
    "            all_reviews.append(current_review)\n",
    "\n",
    "            #print(current_review)\n",
    "        \n",
    "        # See if there is another page of reviews\n",
    "        if (review_soup.find_all('a', {'class':'next_page'}) != []):\n",
    "            # There is another page\n",
    "            reviews_url = 'https://goodreads.com' + review_soup.find('a', {'class':'next_page'})['href']\n",
    "            response_reviews = requests.get(reviews_url)\n",
    "        else:\n",
    "            # No more pages of reviews\n",
    "            break\n",
    "        \n",
    "    return all_reviews\n",
    "\n",
    "# Save all reviews as a CSV\n",
    "def download_reviews(api_key):\n",
    "    data_columns = ['book_id', 'rating', 'date', 'text']\n",
    "    \n",
    "    with open('reviews.csv', 'w', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, data_columns, lineterminator='\\n')\n",
    "        dict_writer.writeheader()\n",
    "        \n",
    "        # TODO: iterate through book IDs - add a time.sleep(1) between each iteration\n",
    "        current_reviews = get_reviews(api_key, 68428)\n",
    "        dict_writer.writerows(current_reviews)\n",
    "\n",
    "#book_reviews = get_reviews(api_key, 68428)\n",
    "#print(len(book_reviews))\n",
    "download_reviews(api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
